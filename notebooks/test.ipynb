{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プレイの様子を動画で見てみるための関数\n",
    "def display_video(frames):\n",
    "    plt.figure(figsize=(8, 8), dpi=50)\n",
    "    patch = plt.imshow(frames[0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    display(HTML(anim.to_jshtml(default_mode='once')))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\", full_action_space=True)\n",
    "#env = gym.make(\"PooyanNoFrameskip-v4\", full_action_space=True)\n",
    "#Atari preprocessing wrapper\n",
    "env = gym.wrappers.AtariPreprocessing(env, noop_max=30, frame_skip=4, screen_size=84, terminal_on_life_loss=False, grayscale_obs=True, grayscale_newaxis=False, scale_obs=False)\n",
    "#Frame stacking\n",
    "env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "obs = env.reset()\n",
    "frames = []\n",
    "actions = []\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    frames.append(obs[0])\n",
    "    action = env.action_space.sample()  # 行動空間から一様ランダムに行動をサンプル\n",
    "    next_obs, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    obs = next_obs\n",
    "    actions.append(action)\n",
    "\n",
    "print('Reward: ', total_reward)\n",
    "display_video(frames)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    リプレイバッファの宣言\n",
    "\"\"\"\n",
    "buffer_size = 100000  #　リプレイバッファに入る経験の最大数\n",
    "initial_buffer_size = 10000  # 学習を開始する最低限の経験の数\n",
    "replay_buffer = PrioritizedReplayBuffer(buffer_size)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    ネットワークの宣言\n",
    "\"\"\"\n",
    "net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n",
    "target_net = CNNQNetwork(env.observation_space.shape, n_action=env.action_space.n).to(device)\n",
    "target_update_interval = 2000  # 学習安定化のために用いるターゲットネットワークの同期間隔\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    オプティマイザとロス関数の宣言\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)  # オプティマイザはAdam\n",
    "loss_func = nn.SmoothL1Loss(reduction='none')  # ロスはSmoothL1loss（別名Huber loss）\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Prioritized Experience Replayのためのパラメータβ\n",
    "\"\"\"\n",
    "beta_begin = 0.4\n",
    "beta_end = 1.0\n",
    "beta_decay = 500000\n",
    "# beta_beginから始めてbeta_endまでbeta_decayかけて線形に増やす\n",
    "beta_func = lambda step: min(beta_end, beta_begin + (beta_end - beta_begin) * (step / beta_decay))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    探索のためのパラメータε\n",
    "\"\"\"\n",
    "epsilon_begin = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 50000\n",
    "# epsilon_beginから始めてepsilon_endまでepsilon_decayかけて線形に減らす\n",
    "epsilon_func = lambda step: max(epsilon_end, epsilon_begin - (epsilon_begin - epsilon_end) * (step / epsilon_decay))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    その他のハイパーパラメータ\n",
    "\"\"\"\n",
    "gamma = 0.99  #　割引率\n",
    "batch_size = 32\n",
    "n_episodes = 30  # 学習を行うエピソード数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(batch_size, beta):\n",
    "    obs, action, reward, next_obs, done, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "    obs, action, reward, next_obs, done, weights \\\n",
    "        = obs.float().to(device), action.to(device), reward.to(device), next_obs.float().to(device), done.to(device), weights.to(device)\n",
    "\n",
    "    #　ニューラルネットワークによるQ関数の出力から, .gatherで実際に選択した行動に対応する価値を集めてきます.\n",
    "    q_values = net(obs).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # 目標値の計算なので勾配を追跡しない\n",
    "    with torch.no_grad():\n",
    "        # Double DQN.\n",
    "        # ① 現在のQ関数でgreedyに行動を選択し,\n",
    "        greedy_action_next = torch.argmax(net(next_obs), dim=1)\n",
    "        # ②　対応する価値はターゲットネットワークのものを参照します.\n",
    "        q_values_next = target_net(next_obs).gather(1, greedy_action_next.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # ベルマン方程式に基づき, 更新先の価値を計算します.\n",
    "    # (1 - done)をかけているのは, ゲームが終わった後の価値は0とみなすためです.\n",
    "    target_q_values = reward + gamma * q_values_next * (1 - done)\n",
    "\n",
    "    # Prioritized Experience Replayのために, ロスに重み付けを行なって更新します.\n",
    "    optimizer.zero_grad()\n",
    "    loss = (weights * loss_func(q_values, target_q_values)).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #　TD誤差に基づいて, サンプルされた経験の優先度を更新します.\n",
    "    replay_buffer.update_priorities(indices, (target_q_values - q_values).abs().detach().cpu().numpy())\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pruning.slth.edgepopup import modify_module_for_slth\n",
    "net = modify_module_for_slth(net, 0.3)\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "rewards = []\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # ε-greedyで行動を選択\n",
    "        obs_np = np.array(obs)\n",
    "        # NumPy配列をPyTorchのテンソルに変換\n",
    "        obs = torch.tensor(obs_np, dtype=torch.float)\n",
    "        action = net.act(obs.to(device), epsilon_func(step))\n",
    "        # 環境中で実際に行動\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        next_obs_np = np.array(next_obs)\n",
    "        # NumPy配列をPyTorchのテンソルに変換\n",
    "        next_obs = torch.tensor(next_obs_np, dtype=torch.float)\n",
    "\n",
    "        # リプレイバッファに経験を蓄積\n",
    "        replay_buffer.push([obs, action, reward, next_obs, done])\n",
    "        obs = next_obs\n",
    "        obs = obs.cpu()\n",
    "\n",
    "        # ネットワークを更新\n",
    "        if len(replay_buffer) > initial_buffer_size:\n",
    "            update(batch_size, beta_func(step))\n",
    "\n",
    "        # ターゲットネットワークを定期的に同期させる\n",
    "        if (step + 1) % target_update_interval == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print('Episode: {},  Step: {},  Reward: {}'.format(episode + 1, step + 1, total_reward))\n",
    "    rewards.append(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    frames.append(env.render(mode='rgb_array')[:, :, 0])\n",
    "    # テスト時なので, ランダム行動は切る\n",
    "    obs_np = np.array(obs)\n",
    "    # NumPy配列をPyTorchのテンソルに変換\n",
    "    obs = torch.tensor(obs_np, dtype=torch.float).to(device)\n",
    "    action = net.act(obs.float().to(device), epsilon=0.0)\n",
    "    next_obs, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    obs = next_obs\n",
    "\n",
    "print('Reward: ', total_reward)\n",
    "display_video(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
